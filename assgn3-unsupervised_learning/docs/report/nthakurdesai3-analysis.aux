\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{streetNuclearFeatureExtraction1993}
\citation{cortezModelingWinePreferences2009}
\citation{arthurKmeansAdvantagesCareful2007}
\@writefile{toc}{\contentsline {section}{\numberline {1}Datasets}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Dataset 1: Breast Cancer Wisconsin (Diagnostic) Data Set}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Dataset 2: Wine Quality}{1}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Overview of the algorithms}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}k-means clustering}{1}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}GMMs with EM}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}PCA}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and analysis}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Clustering without dimensionality reduction}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}K-means}{2}{subsubsection.3.1.1}}
\citation{rousseeuwSilhouettesGraphicalAid1987}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Inertia for different values of k}}{3}{figure.1}}
\newlabel{fig:kmeans_losses}{{1}{3}{Inertia for different values of k}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Distribution of samples across k-means clusters (before PCA)}}{3}{figure.2}}
\newlabel{fig:kmeans_hist}{{2}{3}{Distribution of samples across k-means clusters (before PCA)}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Distribution of samples across GMM clusters}}{3}{figure.3}}
\newlabel{fig:gmm_hist}{{3}{3}{Distribution of samples across GMM clusters}{figure.3}{}}
\citation{vinhInformationTheoreticMeasures2009}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces K-means visualization (without PCA)}}{4}{figure.4}}
\newlabel{fig:kmeans_viz}{{4}{4}{K-means visualization (without PCA)}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Clustering performance evaluation}}{4}{table.1}}
\newlabel{tab:silhouette_bic1}{{1}{4}{Clustering performance evaluation}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces AMI scores}}{5}{table.2}}
\newlabel{tab:ami1}{{2}{5}{AMI scores}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces GMM visualization (without PCA)}}{5}{figure.5}}
\newlabel{fig:gmm_viz}{{5}{5}{GMM visualization (without PCA)}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}GMM-EM}{5}{subsubsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces BIC for different number of GMM components}}{6}{figure.6}}
\newlabel{fig:gmm_bic}{{6}{6}{BIC for different number of GMM components}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualization of GMM weights (without PCA)}}{6}{figure.7}}
\newlabel{fig:gmm_wts_viz}{{7}{6}{Visualization of GMM weights (without PCA)}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces GMM maximum posterior probabilities (without PCA)}}{6}{figure.8}}
\newlabel{fig:gmm_probs_viz}{{8}{6}{GMM maximum posterior probabilities (without PCA)}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Clustering with dimensionality reduction}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}PCA}{6}{subsubsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Variance for different components}}{7}{figure.9}}
\newlabel{fig:pca_var}{{9}{7}{Variance for different components}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Cumulative variance}}{7}{figure.10}}
\newlabel{fig:pca_cum_var}{{10}{7}{Cumulative variance}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Distribution of samples across k-means clusters (after PCA)}}{7}{figure.11}}
\newlabel{fig:pca_kmeans_hist}{{11}{7}{Distribution of samples across k-means clusters (after PCA)}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Distribution of samples across GMM clusters (after PCA)}}{7}{figure.12}}
\newlabel{fig:pca_gmm_hist}{{12}{7}{Distribution of samples across GMM clusters (after PCA)}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}K-means}{7}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}GMM-EM}{7}{subsubsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces K-means visualization (after PCA)}}{8}{figure.13}}
\newlabel{fig:pca_kmeans_viz}{{13}{8}{K-means visualization (after PCA)}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces GMM visualization (after PCA)}}{8}{figure.14}}
\newlabel{fig:pca_gmm_viz}{{14}{8}{GMM visualization (after PCA)}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Visualization of GMM weights (with PCA)}}{8}{figure.15}}
\newlabel{fig:pca_gmm_wts_viz}{{15}{8}{Visualization of GMM weights (with PCA)}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces GMM maximum posterior probabilities (with PCA)}}{8}{figure.16}}
\newlabel{fig:pca_gmm_probs_viz}{{16}{8}{GMM maximum posterior probabilities (with PCA)}{figure.16}{}}
\bibstyle{unsrt}
\bibdata{cs7641-a3}
\bibcite{streetNuclearFeatureExtraction1993}{1}
\bibcite{cortezModelingWinePreferences2009}{2}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance metrics for different dimensionality reduction algorithms}}{9}{table.3}}
\newlabel{tab:nn}{{3}{9}{Performance metrics for different dimensionality reduction algorithms}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural network classification after dimensionality reduction}{9}{subsection.3.3}}
\bibcite{arthurKmeansAdvantagesCareful2007}{3}
\bibcite{rousseeuwSilhouettesGraphicalAid1987}{4}
\bibcite{vinhInformationTheoreticMeasures2009}{5}
